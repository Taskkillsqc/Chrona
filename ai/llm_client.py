"""
LLM å®¢æˆ·ç«¯æ¨¡å— - V3 ç‰ˆæœ¬
æ”¯æŒå¤šç§ LLM æä¾›å•†ã€è‡ªå®šä¹‰é…ç½®å’Œæœ¬åœ°æ¨¡å‹
"""

import requests
import json
import os
from typing import Dict, Any, Optional

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æä¾›å•†"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.llm_config = self._parse_config()
        self.local_model = None  # ç”¨äºç¼“å­˜æœ¬åœ°æ¨¡å‹å®ä¾‹
        
    def _parse_config(self) -> Dict[str, Any]:
        """è§£æé…ç½®ï¼Œæ”¯æŒæ–°æ—§æ ¼å¼"""
        # ä¼˜å…ˆä½¿ç”¨æ–°çš„ llm é…ç½®
        if 'llm' in self.config:
            llm_config = self.config['llm']
              # å¦‚æœå¯ç”¨äº†æœ¬åœ°æ¨¡å‹
            if llm_config.get('local', {}).get('enabled', False):
                local_config = llm_config['local']
                model_path = local_config.get('model_path')
                if not model_path or not os.path.exists(model_path):
                    print(f"âš ï¸ è­¦å‘Š: æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                
                return {
                    'provider': 'local',
                    'model_path': model_path,
                    'context_length': local_config.get('context_length', 2048),
                    'gpu_layers': local_config.get('gpu_layers', 0),
                    'n_threads': local_config.get('n_threads', None),
                    'verbose': local_config.get('verbose', False),
                    'parameters': {
                        'temperature': local_config.get('temperature', 0.7),
                        'max_tokens': local_config.get('max_tokens', 1000),
                        'top_p': local_config.get('top_p', 0.9),
                        'top_k': local_config.get('top_k', 40),
                        'repeat_penalty': local_config.get('repeat_penalty', 1.1)
                    }
                }
            # å¦‚æœå¯ç”¨äº†è‡ªå®šä¹‰é…ç½®
            elif llm_config.get('custom', {}).get('enabled', False):
                return {
                    'provider': 'custom',
                    'url': llm_config['custom']['url'],
                    'model': llm_config['custom']['model'],
                    'headers': llm_config['custom'].get('headers', {}),
                    'payload_format': llm_config['custom'].get('payload_format', 'openai'),
                    'response_format': llm_config['custom'].get('response_format', 'openai'),
                    'timeout': llm_config['custom'].get('timeout', 30),
                    'parameters': llm_config.get('parameters', {})
                }
            else:
                # ä½¿ç”¨é¢„è®¾æä¾›å•†
                provider = llm_config.get('provider', 'gemini')  # é»˜è®¤ gemini
                api_key = llm_config.get('api_key')
                
                # æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å­˜åœ¨
                if not api_key:
                    print(f"âš ï¸ è­¦å‘Š: {provider} æä¾›å•†çš„ API å¯†é’¥æœªé…ç½®")
                
                return {
                    'provider': provider,
                    'api_key': api_key,
                    'parameters': llm_config.get('parameters', {}),
                    'timeout': 30
                }
        else:
            # å‘åå…¼å®¹æ—§é…ç½®
            provider = self.config.get('model', 'gemini')  # é»˜è®¤ gemini
            api_key = self.config.get('api_key')
            
            # æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å­˜åœ¨
            if not api_key:
                print(f"âš ï¸ è­¦å‘Š: {provider} æä¾›å•†çš„ API å¯†é’¥æœªé…ç½®")
            
            return {
                'provider': provider,
                'api_key': api_key,                'parameters': {},
                'timeout': 30
            }
    
    def generate(self, prompt: str) -> Dict[str, Any]:
        """ç”Ÿæˆå›å¤"""
        try:
            if self.llm_config['provider'] == 'local':
                return self._call_local(prompt)
            elif self.llm_config['provider'] == 'gemini':
                return self._call_gemini(prompt)
            elif self.llm_config['provider'] == 'deepseek':
                return self._call_deepseek(prompt)
            elif self.llm_config['provider'] == 'openai':
                return self._call_openai(prompt)
            elif self.llm_config['provider'] == 'custom':
                return self._call_custom(prompt)
            else:
                return {"error": f"ä¸æ”¯æŒçš„æä¾›å•†: {self.llm_config['provider']}"}
                
        except Exception as e:
            return {"error": f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"}
    
    def _call_gemini(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨ Gemini API"""
        api_key = self.llm_config.get('api_key')
        if not api_key:
            return {"error": "Gemini APIå¯†é’¥æœªé…ç½®"}
            
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={api_key}"
        
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": self.llm_config['parameters'].get('temperature', 0.7),
                "maxOutputTokens": self.llm_config['parameters'].get('max_tokens', 1000),
                "topP": self.llm_config['parameters'].get('top_p', 0.9)
            }
        }
        
        headers = {"Content-Type": "application/json"}
        
        response = requests.post(
            url, 
            headers=headers, 
            json=payload, 
            timeout=self.llm_config['timeout']
        )
        
        if response.status_code == 200:
            data = response.json()
            text = data["candidates"][0]["content"]["parts"][0]["text"]
            return {"success": True, "text": text}
        else:
            return {"error": f"Gemini APIè¯·æ±‚å¤±è´¥: {response.status_code}", "raw": response.text}
    
    def _call_deepseek(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨ DeepSeek API"""
        api_key = self.llm_config.get('api_key')
        if not api_key:
            return {"error": "DeepSeek APIå¯†é’¥æœªé…ç½®"}
            
        url = "https://api.deepseek.com/chat/completions"
        
        payload = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.llm_config['parameters'].get('temperature', 0.7),
            "max_tokens": self.llm_config['parameters'].get('max_tokens', 1000),
            "top_p": self.llm_config['parameters'].get('top_p', 0.9)
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        response = requests.post(
            url, 
            headers=headers, 
            json=payload, 
            timeout=self.llm_config['timeout']
        )
        
        if response.status_code == 200:
            data = response.json()
            text = data['choices'][0]['message']['content']
            return {"success": True, "text": text}
        else:
            return {"error": f"DeepSeek APIè¯·æ±‚å¤±è´¥: {response.status_code}", "raw": response.text}
    
    def _call_openai(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨ OpenAI API"""
        api_key = self.llm_config.get('api_key')
        if not api_key:
            return {"error": "OpenAI APIå¯†é’¥æœªé…ç½®"}
            
        url = "https://api.openai.com/v1/chat/completions"
        
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.llm_config['parameters'].get('temperature', 0.7),
            "max_tokens": self.llm_config['parameters'].get('max_tokens', 1000),
            "top_p": self.llm_config['parameters'].get('top_p', 0.9)
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        response = requests.post(
            url, 
            headers=headers, 
            json=payload, 
            timeout=self.llm_config['timeout']
        )
        
        if response.status_code == 200:
            data = response.json()
            text = data['choices'][0]['message']['content']
            return {"success": True, "text": text}
        else:
            return {"error": f"OpenAI APIè¯·æ±‚å¤±è´¥: {response.status_code}", "raw": response.text}
    
    def _call_custom(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨è‡ªå®šä¹‰ API"""
        url = self.llm_config.get('url')
        if not url:
            return {"error": "è‡ªå®šä¹‰API URLæœªé…ç½®"}
        
        # æ„å»ºè¯·æ±‚å¤´
        headers = {"Content-Type": "application/json"}
        custom_headers = self.llm_config.get('headers', {})
        headers.update(custom_headers)
        
        # æ„å»ºè¯·æ±‚ä½“
        payload_format = self.llm_config.get('payload_format', 'openai')
        
        if payload_format == 'openai':
            payload = {
                "model": self.llm_config.get('model', 'gpt-3.5-turbo'),
                "messages": [{"role": "user", "content": prompt}],
                "temperature": self.llm_config['parameters'].get('temperature', 0.7),
                "max_tokens": self.llm_config['parameters'].get('max_tokens', 1000),
                "top_p": self.llm_config['parameters'].get('top_p', 0.9)
            }
        else:
            # è‡ªå®šä¹‰æ ¼å¼ï¼Œç”¨æˆ·éœ€è¦åœ¨é…ç½®ä¸­å®šä¹‰
            payload = {
                "prompt": prompt,
                "model": self.llm_config.get('model'),
                **self.llm_config['parameters']
            }
        
        response = requests.post(
            url, 
            headers=headers, 
            json=payload, 
            timeout=self.llm_config['timeout']
        )
        
        if response.status_code == 200:
            data = response.json()
            
            # è§£æå“åº”
            response_format = self.llm_config.get('response_format', 'openai')
            
            if response_format == 'openai':
                text = data['choices'][0]['message']['content']
            else:
                # è‡ªå®šä¹‰å“åº”æ ¼å¼ï¼Œå°è¯•å¸¸è§å­—æ®µ
                text = data.get('text') or data.get('content') or data.get('response')
                if not text:
                    return {"error": "æ— æ³•è§£æè‡ªå®šä¹‰APIå“åº”", "raw": data}            
            return {"success": True, "text": text}
        else:
            return {"error": f"è‡ªå®šä¹‰APIè¯·æ±‚å¤±è´¥: {response.status_code}", "raw": response.text}
    
    def _call_local(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨æœ¬åœ° GGUF æ¨¡å‹"""
        try:
            # å»¶è¿Ÿå¯¼å…¥ llama-cpp-pythonï¼Œé¿å…åœ¨ä¸ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ—¶çš„ä¾èµ–é—®é¢˜
            from llama_cpp import Llama
        except ImportError:
            return {
                "error": "llama-cpp-python æœªå®‰è£…",
                "details": [
                    "æœ¬åœ° LLM éœ€è¦ llama-cpp-python åº“æ”¯æŒ",
                    "Windows å®‰è£…è¯´æ˜:",
                    "1. å®‰è£… Visual Studio Build Tools æˆ– Visual Studio Community",
                    "2. å®‰è£… CMake: https://cmake.org/download/",
                    "3. è¿è¡Œ: pip install llama-cpp-python",
                    "æˆ–è€…ä½¿ç”¨é¢„ç¼–è¯‘ç‰ˆæœ¬: pip install llama-cpp-python --prefer-binary",
                    "è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ: models/README.md"
                ]
            }
        
        model_path = self.llm_config.get('model_path')
        if not model_path or not os.path.exists(model_path):
            return {"error": f"æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"}
        
        try:
            # å¦‚æœæ¨¡å‹è¿˜æœªåŠ è½½ï¼Œåˆ™åŠ è½½æ¨¡å‹
            if self.local_model is None:
                print(f"ğŸ¤– æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {os.path.basename(model_path)}")
                self.local_model = Llama(
                    model_path=model_path,
                    n_ctx=self.llm_config.get('context_length', 2048),
                    n_gpu_layers=self.llm_config.get('gpu_layers', 0),
                    n_threads=self.llm_config.get('n_threads', None),
                    verbose=self.llm_config.get('verbose', False)
                )
                print(f"âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ")
              # ç”Ÿæˆå›å¤
            params = self.llm_config.get('parameters', {})
            response = self.local_model(
                prompt,
                max_tokens=params.get('max_tokens', 1000),
                temperature=params.get('temperature', 0.7),
                top_p=params.get('top_p', 0.9),
                top_k=params.get('top_k', 40),
                repeat_penalty=params.get('repeat_penalty', 1.1),
                stop=["</s>", "<|im_end|>", "<|endoftext|>", "\n\n", "```", "---"]  # æ‰©å±•åœæ­¢æ ‡è®°ï¼Œé¿å…è¿‡åº¦ç”Ÿæˆ
            )
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            if isinstance(response, dict) and 'choices' in response:
                text = response['choices'][0]['text'].strip()
            else:
                text = str(response).strip()
            
            return {"success": True, "text": text}
            
        except Exception as e:
            return {"error": f"æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}"}
    
    def get_provider_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰æä¾›å•†ä¿¡æ¯"""
        return {
            "provider": self.llm_config['provider'],
            "model": self.llm_config.get('model', 'N/A'),
            "url": self.llm_config.get('url', 'N/A'),
            "parameters": self.llm_config.get('parameters', {})
        }
